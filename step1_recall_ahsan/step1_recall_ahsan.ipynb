{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import pickle\n",
    "from openai import AzureOpenAI\n",
    "import sys\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_ID_COLUMN_NAME = \"ROW_ID\"\n",
    "UNIQUE_TEXT_COLUMN_NAME = \"TEXT\"\n",
    "UNIQUE_LABEL_COLUMN_NAMES = ['sdoh_economics','sdoh_environment']\n",
    "# Economics (0: None, 1: True[Non-Adverse], 2: False[Adverse])\n",
    "# Environment (0: None, 1: True[Non-Adverse], 2: False[Adverse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_social_history(df):\n",
    "    replace_texts = []\n",
    "    for row_id in df[UNIQUE_ID_COLUMN_NAME]:\n",
    "        patient = df[df[UNIQUE_ID_COLUMN_NAME] == row_id][UNIQUE_TEXT_COLUMN_NAME].iloc[0]\n",
    "        social_history_start = patient.lower().find('social history:')\n",
    "        pos_ends = []\n",
    "        pos_ends.append(patient.lower().find('family history:'))\n",
    "        pos_ends.append(patient.lower().find('physical exam'))\n",
    "        pos_ends.append(patient.lower().find('medications:'))\n",
    "        pos_ends.append(patient.lower().find('hospital course:'))\n",
    "        pos_ends.append(patient.lower().find('review of systems:'))\n",
    "        pos_ends = [x for x in pos_ends if x > social_history_start]\n",
    "        pos_ends.append(social_history_start+500)\n",
    "        social_history_end = min(pos_ends)\n",
    "        replace_texts.append((row_id,patient[social_history_start:social_history_end]))\n",
    "    texts = pd.DataFrame(replace_texts,columns =[UNIQUE_ID_COLUMN_NAME,UNIQUE_TEXT_COLUMN_NAME])\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to MIMIC_CSVs\n",
    "MIMIC_ADMISSION_CSV = \"../ahsan_data/ADMISSIONS.csv\" #Fill in path/to/file with the path to your MIMIC-III folder\n",
    "MIMIC_NOTEEVENTS_CSV = \"../ahsan_data/NOTEEVENTS.csv\" #Fill in path/to/file with the path to your MIMIC-III folder\n",
    "MIMIC_SBDH = \"../ahsan_data/MIMIC-SBDH.csv\" #Fill in path/to/file with the path to your MIMIC-SBDH folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(MIMIC_ADMISSION_CSV)\n",
    "notes_df = pd.read_csv(MIMIC_NOTEEVENTS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading DataFrames for Annotated and Unnanotated MIMIC Notes\n",
    "\n",
    "newborn_list = df[df[\"ADMISSION_TYPE\"] == \"NEWBORN\"].SUBJECT_ID.to_list()\n",
    "discharge_df = notes_df[notes_df['CATEGORY'] == 'Discharge summary']\n",
    "non_neonatal = discharge_df[~discharge_df['SUBJECT_ID'].isin(newborn_list)]\n",
    "\n",
    "sbdh_data = pd.read_csv(open(MIMIC_SBDH, 'r+', encoding='UTF-8'),encoding='UTF-8', on_bad_lines='warn')\n",
    "sbdh_data = sbdh_data.rename(columns={'row_id':UNIQUE_ID_COLUMN_NAME})\n",
    "\n",
    "annotated_list = sbdh_data[UNIQUE_ID_COLUMN_NAME].tolist()\n",
    "annotated_notes = discharge_df[discharge_df[UNIQUE_ID_COLUMN_NAME].isin(annotated_list)]\n",
    "annotated_subjects = discharge_df[discharge_df[UNIQUE_ID_COLUMN_NAME].isin(annotated_list)].SUBJECT_ID.to_list()\n",
    "\n",
    "no_soc_his = []\n",
    "for index, row in non_neonatal.iterrows():\n",
    "    if 'social history:' not in row[UNIQUE_TEXT_COLUMN_NAME].lower():\n",
    "        no_soc_his.append(row[UNIQUE_ID_COLUMN_NAME])\n",
    "\n",
    "final_sdoh_list = non_neonatal[~non_neonatal[UNIQUE_ID_COLUMN_NAME].isin(no_soc_his)]\n",
    "unnanotated_notes = final_sdoh_list[~final_sdoh_list[UNIQUE_ID_COLUMN_NAME].isin(annotated_list)]\n",
    "\n",
    "annotated_sh = retrieve_social_history(annotated_notes)\n",
    "annotated_sh = pd.merge(annotated_sh,sbdh_data[[UNIQUE_ID_COLUMN_NAME] + UNIQUE_LABEL_COLUMN_NAMES],on=UNIQUE_ID_COLUMN_NAME, how='left')\n",
    "unannotated_sh = retrieve_social_history(unnanotated_notes)\n",
    "\n",
    "df = newborn_list = notes_df = discharge_df = non_neonatal = annotated_list = annotated_subjects = no_soc_his = final_sdoh_list = unnanotated = sbdh_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Azure Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../azure_credentials.json', 'r') as file:\n",
    "    azure_data = json.load(file)\n",
    "    api_key = azure_data['API_KEY']\n",
    "    api_version = azure_data['API_VERSION']\n",
    "    azure_endpoint = azure_data['AZURE_ENDPOINT']\n",
    "    azure_deployment_name = azure_data['AZURE_DEPLOYMENT_NAME']\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint = azure_endpoint\n",
    "    )\n",
    "\n",
    "deployment_name=azure_deployment_name #This will correspond to the custom name you chose for your deployment when you deployed a model. Use a gpt-35-turbo-instruct deployment.\n",
    "\n",
    "# Defining a function to create the prompt from the instruction system message, the few-shot examples, and the current query\n",
    "def create_prompt(system_message, user_message):    \n",
    "    formatted_message = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    return formatted_message\n",
    "\n",
    "# This function sends the prompt to the GPT model\n",
    "def send_message(message, model_name, max_response_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=message,\n",
    "        temperature=0,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting SDoH from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sh['sdoh_environment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "llm_response_list = []\n",
    "filtered_df = annotated_sh[(annotated_sh['sdoh_environment'] == 1) | (annotated_sh['sdoh_environment'] == 2)]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "system_message = \"You are an information extract tool that follows instructions very well and is specifically trained to extract social determinants of health elements from hospital generated free-text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "current_index = start_index\n",
    "# total_records = len(filtered_df)\n",
    "total_records = 10\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        for index in range(start_index, total_records):\n",
    "            current_index = index\n",
    "            row = filtered_df.iloc[current_index, :]\n",
    "            free_text = row['TEXT']\n",
    "            user_message = step1_query_ahsan.format(free_text=free_text)\n",
    "            openai_message = create_prompt(system_message, user_message)\n",
    "            response = send_message(openai_message, deployment_name)\n",
    "            \n",
    "            index_list.append(current_index)\n",
    "            llm_response_list.append(response)\n",
    "            print(current_index)\n",
    "            print(free_text)\n",
    "            print(response)\n",
    "            print()\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Something went wrong: \", err)\n",
    "        start_index = current_index\n",
    "        print(\"Waiting for 10 seconds before continuing again with index:\", start_index)\n",
    "        time.sleep(10)\n",
    "\n",
    "    # Break the loop if current_index has completed\n",
    "    if current_index == (total_records - 1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_environment_adverse_nonadverse_step1 = pd.DataFrame({'index': index_list, 'llm_environment_adverse_nonadverse_step1': llm_response_list})\n",
    "\n",
    "file_name = 'llm_environment_adverse_nonadverse_step1_' + str(start_index) + '_' + str(total_records) + '.pkl'\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(llm_environment_adverse_nonadverse_step1, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdoh-guevera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
